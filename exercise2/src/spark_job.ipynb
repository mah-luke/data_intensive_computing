{
 "cells": [
  {
   "cell_type": "raw",
   "id": "833e2766",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'Part 1: RDDs'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d3b5c",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Before starting with the tasks of the assignment, initialize spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fd54e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import RDD, SparkContext\n",
    "\n",
    "from exercise2.model.review import Review\n",
    "from exercise2.split_text import split_text\n",
    "from exercise2.task1.util import calculate_chi_squares, merge_dicts, printable_category, calculate_chi_square_per_token\n",
    "\n",
    "\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .appName(\"poc\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "sc: SparkContext = spark.sparkContext\n",
    "# conf = SparkConf().setAppName(\"poc\") \\\n",
    "#     .setMaster(\"local[1]\")\n",
    "# conf.set(\"spark.driver.host\", \"localhost\")\n",
    "#\n",
    "# sc = SparkContext(conf=conf)\n",
    "BASE_PATH = Path().resolve()\n",
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a769e",
   "metadata": {},
   "source": [
    "# Example 1: RDD\n",
    "Redo the first assignment, this time utilizing RDDs. \n",
    "\n",
    "Start by loading the reviews dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5274b32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "path = BASE_PATH / \"resource\" / \"reviews_devset.json\" #_first1000.json\" #\"reviews_devset.json\"\n",
    "reviews: RDD[Review] = sc.textFile(str(path)).map(json.loads)\n",
    "reviews_cnt = reviews.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67119922",
   "metadata": {},
   "source": [
    "Once the reviews are available, count the documents per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = reviews.map(lambda r: (r[\"category\"], 1)) \\\n",
    "        .reduceByKey(operator.add) \\\n",
    "        .collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aad00e",
   "metadata": {},
   "source": [
    "Split the reviews text into separate tokens, filter them using the stopwords (loaded from disk)\n",
    "and reduce the produced counts into a map of maps with token on the top level, each token is \n",
    "assigned to a map, which contains the review counts for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17351448",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_PATH / \"resource\" / \"stopwords.txt\", \"r\") as file:\n",
    "    stopwords = set([line.strip() for line in file.readlines()])\n",
    "\n",
    "# <term>: {<cat>: cnt, <cat>: cnt, ...}\n",
    "category_counts_per_token = reviews \\\n",
    "        .flatMap(lambda r: ([(token, r[\"category\"]) for token in set(split_text(r[\"reviewText\"]))])) \\\n",
    "        .filter(lambda r: r[1] not in stopwords) \\\n",
    "        .mapValues(lambda category: {category: 1}) \\\n",
    "        .reduceByKey(lambda dict1, dict2: merge_dicts(dict1, dict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80154c",
   "metadata": {},
   "source": [
    "Now that we have the counts per token and category, we calculate the chi square values and sort\n",
    "the values to filter for the top 75 tokens per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top75_tokens_per_category: RDD = category_counts_per_token \\\n",
    "        .flatMap(lambda cur_category_counts: calculate_chi_square_per_token(cur_category_counts)) \\\n",
    "        .groupByKey().mapValues(list) \\\n",
    "        .mapValues(lambda val: sorted(val, key=lambda val: val[1])[:-75:-1]) \\\n",
    "        .sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1352567b",
   "metadata": {},
   "source": [
    "Prepare the job result by concenating all tokens to a list of all top tokens \n",
    "and convert the lists to strings for printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97121347",
   "metadata": {},
   "outputs": [],
   "source": [
    "top75_tokens_str = \"\\n\".join(\n",
    "        top75_tokens_per_category.map(lambda el: printable_category(el[0], el[1])).collect())\n",
    "\n",
    "top_tokens: list[str] = top75_tokens_per_category \\\n",
    "        .flatMap(lambda el: [tup[0] for tup in el[1]]) \\\n",
    "        .distinct() \\\n",
    "        .sortBy(lambda el: el).collect()\n",
    "top_tokens_str = \" \".join(top_tokens)\n",
    "\n",
    "result = top_tokens_str + '\\n' + top75_tokens_str\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7db0a",
   "metadata": {},
   "source": [
    "Write the result to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b108d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_PATH / \"output_rdd.txt\", \"w\") as file:\n",
    "    file.writelines(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170af5c",
   "metadata": {},
   "source": [
    "# Example 2: "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all"
  },
  "kernelspec": {
   "display_name": "spark_job",
   "language": "python",
   "name": "spark_job"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

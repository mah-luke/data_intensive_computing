{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c11d3b5c",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Before starting with the tasks of the assignment, initialize spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fd54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "p = Path().resolve()\n",
    "BASE_PATH =  p.parent if p.name == \"src\" else p\n",
    "\n",
    "# To be sure that the src path is in PYTHONPATH\n",
    "sys.path.append(str(BASE_PATH / \"src\"))\n",
    "\n",
    "import operator\n",
    "import json\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import RDD, SparkContext\n",
    "from pyspark.ml.feature import (\n",
    "    IDF,\n",
    "    ChiSqSelector,\n",
    "    ChiSqSelectorModel,\n",
    "    CountVectorizer,\n",
    "    RegexTokenizer,\n",
    "    HashingTF,\n",
    "    StopWordsRemover,\n",
    "    StringIndexer,\n",
    "    VectorIndexer,\n",
    "    PCA,\n",
    ")\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from exercise2.model.review import Review\n",
    "from exercise2.split_text import split_text\n",
    "from exercise2.task1.util import calculate_chi_squares, merge_dicts, printable_category, calculate_chi_square_per_token\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LOCAL = True\n",
    "\n",
    "if LOCAL:\n",
    "    spark: SparkSession = SparkSession.builder \\\n",
    "        .appName(\"local\") \\\n",
    "        .config(\"spark.driver.host\", \"localhost\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"localhost\") \\\n",
    "        .getOrCreate()\n",
    "    sc: SparkContext = spark.sparkContext\n",
    "else:\n",
    "    spark: SparkSession = SparkSession.builder \\\n",
    "        .appName(\"cluster\") \\\n",
    "        .config(\"spark.executor.instances\", 435) \\\n",
    "        .getOrCreate()\n",
    "    sc: SparkContext = spark.sparkContext\n",
    "    sc.addPyFile(str(BASE_PATH / \"src\" / \"exercise2.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a769e",
   "metadata": {},
   "source": [
    "# Task 1: RDD\n",
    "Redo the first assignment, this time utilizing RDDs. \n",
    "\n",
    "Start by loading the reviews dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5274b32",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOCAL:\n",
    "    reviews_location = BASE_PATH / \"resource\" / \"reviews_devset_rand100.json\" #_first1000.json\" #\"reviews_devset.json\"\n",
    "else:\n",
    "    # reviews_location = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "    reviews_location = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "reviews: RDD = sc.textFile(str(reviews_location)).map(json.loads)\n",
    "reviews_cnt = reviews.count()\n",
    "\n",
    "print(f\"Number of Reviews in this dataset: {reviews_cnt}\")\n",
    "reviews.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67119922",
   "metadata": {},
   "source": [
    "Once the reviews are available, count the documents per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = reviews.map(lambda r: (r[\"category\"], 1)) \\\n",
    "        .reduceByKey(operator.add) \\\n",
    "        .collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aad00e",
   "metadata": {},
   "source": [
    "Split the reviews text into separate tokens, filter them using the stopwords (loaded from disk)\n",
    "and reduce the produced counts into a map of maps with token on the top level, each token is \n",
    "assigned to a map, which contains the review counts for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17351448",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_PATH / \"src\" / \"stopwords.txt\", \"r\") as file:\n",
    "    stopwords = set([line.strip() for line in file.readlines()])\n",
    "\n",
    "# <term>: {<cat>: cnt, <cat>: cnt, ...}\n",
    "category_counts_per_token = reviews \\\n",
    "        .flatMap(lambda r: ([(token, r[\"category\"]) for token in set(split_text(r[\"reviewText\"]))])) \\\n",
    "        .filter(lambda r: r[1] not in stopwords) \\\n",
    "        .mapValues(lambda category: {category: 1}) \\\n",
    "        .reduceByKey(lambda dict1, dict2: merge_dicts(dict1, dict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80154c",
   "metadata": {},
   "source": [
    "Now that we have the counts per token and category, we calculate the chi square values and sort\n",
    "the values to filter for the top 75 tokens per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top75_tokens_per_category: RDD = category_counts_per_token \\\n",
    "        .flatMap(lambda cur_category_counts: calculate_chi_square_per_token(cur_category_counts, category_counts, reviews_cnt)) \\\n",
    "        .groupByKey().mapValues(list) \\\n",
    "        .mapValues(lambda val: sorted(val, key=lambda val: val[1])[:-75:-1]) \\\n",
    "        .sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1352567b",
   "metadata": {},
   "source": [
    "Prepare the job result by concatenating all tokens to a list of all top tokens \n",
    "and convert the lists to strings for printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97121347",
   "metadata": {},
   "outputs": [],
   "source": [
    "top75_tokens_str = \"\\n\".join(\n",
    "        top75_tokens_per_category.map(lambda el: printable_category(el[0], el[1])).collect())\n",
    "\n",
    "top_tokens: list[str] = top75_tokens_per_category \\\n",
    "        .flatMap(lambda el: [tup[0] for tup in el[1]]) \\\n",
    "        .distinct() \\\n",
    "        .sortBy(lambda el: el).collect()\n",
    "top_tokens_str = \" \".join(top_tokens)\n",
    "\n",
    "result = top_tokens_str + '\\n' + top75_tokens_str\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7db0a",
   "metadata": {},
   "source": [
    "Write the result to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b108d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_PATH / \"output_rdd.txt\", \"w\") as file:\n",
    "    file.writelines(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170af5c",
   "metadata": {},
   "source": [
    "# Task 2: DataFrames: Spark ML & Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82365b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = spark.read.json(str(reviews_location))\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b645570",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(\n",
    "    minTokenLength=2,\n",
    "    pattern=\"[\\s\\d\\(\\)\\[\\]{}\\.!\\?,;:\\+=\\-_\\\"'`~#@&\\*%€\\$§\\\\\\/]\",\n",
    "    inputCol=\"reviewText\",\n",
    "    outputCol=\"tokens\",\n",
    ")\n",
    "stopwords_remover = StopWordsRemover(\n",
    "    inputCol=tokenizer.getOutputCol(), outputCol=\"stopWordsFiltered\"\n",
    ")\n",
    "indexer = StringIndexer(\n",
    "    inputCol=\"category\",\n",
    "    outputCol=\"indexedCategory\"\n",
    ")\n",
    "vectorizer = CountVectorizer(inputCol=stopwords_remover.getOutputCol(), outputCol=\"TF\")\n",
    "idf = IDF(inputCol=vectorizer.getOutputCol(), outputCol=\"TFIDF\")\n",
    "chiSqSelector = ChiSqSelector(\n",
    "    numTopFeatures=2000,\n",
    "    featuresCol=idf.getOutputCol(),\n",
    "    labelCol=indexer.getOutputCol(),\n",
    "    outputCol=\"chiSq\",\n",
    ")\n",
    "pca = PCA(k=10, inputCol=\"chiSq\", outputCol=\"pca_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919e54c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        tokenizer,\n",
    "        stopwords_remover,\n",
    "        indexer,\n",
    "        vectorizer,\n",
    "        idf,\n",
    "        chiSqSelector,\n",
    "        pca\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model = pipeline.fit(reviews_df)\n",
    "tokenized = model.transform(reviews_df)\n",
    "tokenized.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269fedb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Extract the selected tokens by looking up the selected features of the ChiSqSelectorModel.\n",
    "Those features have been mapped by CountVectorizer to integers, hence the selected tokens are retrieved\n",
    "by looking up the index in the vocabulary of the VectorizerModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary: list[str] = model.stages[3].vocabulary\n",
    "chiSqSelectorModel: ChiSqSelectorModel = model.stages[5]\n",
    "selected_tokens = sorted([vocabulary[i] for i in chiSqSelectorModel.selectedFeatures])\n",
    "\n",
    "with open(BASE_PATH / \"output_ds.txt\", \"w\") as file:\n",
    "    file.writelines(\" \".join(selected_tokens))\n",
    "\" \".join(selected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72375a06-9f30-4b22-a50b-6190b8c85bed",
   "metadata": {},
   "source": [
    "# Task 3: Create SVM to predict the category of a review\n",
    "\n",
    "First we split the data in a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1eab4-72fb-47bb-8f95-26e03dc25edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = tokenized.randomSplit([8.0, 2.0], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efd06da-6ed1-43a5-80db-a4fbf61210d3",
   "metadata": {},
   "source": [
    "The chunk below sets up the cross validation pipeline.\n",
    "We first set the Support Vector Machine and to use it with a multi classification problem, One vs Rest is used.\n",
    "For the Cross validation we use the F1 Score to optimize,\n",
    "The final step of setup is creating the grid according to the task description.\n",
    "Finally we can create the cross validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6712ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_num_folds = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c97589",
   "metadata": {},
   "source": [
    "The cell below does not run since LinearSVC + OneVsRest does not play nice with CrossValidator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2fafb6-d3ef-4077-9714-570d98715c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = LinearSVC()\n",
    "# ovr = OneVsRest(classifier=svc, featuresCol=\"chiSq\", labelCol=\"indexedCategory\")\n",
    "# f1_score = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol=\"indexedCategory\")\n",
    "\n",
    "# param_grid = ParamGridBuilder().addGrid(svc.maxIter, [10, 100]).addGrid(svc.regParam, [0, 0.01, 0.1]).addGrid(svc.standardization, [False, True]).build()\n",
    "# cv = CrossValidator(estimator=ovr, estimatorParamMaps=param_grid, evaluator=f1_score, numFolds=cv_num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8360d",
   "metadata": {},
   "source": [
    "On it's own the OneVsRest classifier can be trained and used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd4aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC()\n",
    "ovr = OneVsRest(classifier=svc, featuresCol=\"chiSq\", labelCol=\"indexedCategory\")\n",
    "\n",
    "class_model = ovr.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11edeae",
   "metadata": {},
   "source": [
    "On the other hand the CrossValidator works with other binary classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d2afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "ovr = OneVsRest(classifier=lr, featuresCol=\"chiSq\", labelCol=\"indexedCategory\")\n",
    "f1_score = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol=\"indexedCategory\")\n",
    "param_grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
    "cv = CrossValidator(estimator=ovr, estimatorParamMaps=param_grid, evaluator=f1_score, numFolds=cv_num_folds, seed=1)\n",
    "\n",
    "class_model = cv.fit(train_df)\n",
    "best_model = class_model.bestModel\n",
    "\n",
    "train_res = best_model.transform(train_df)\n",
    "res = f1_score.evaluate(train_res)\n",
    "\n",
    "print(f\"The F1 score is {res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986a6f0",
   "metadata": {},
   "source": [
    "To this end we'd have to do grid search and cross validation manually, which cannot compete in regards of performance with the Pyspark library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591bec9",
   "metadata": {},
   "source": [
    "No matter which classifier has been fitted to the training data, we can then use it to make predictions on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = class_model.transform(train_df)\n",
    "test_res = class_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cd625",
   "metadata": {},
   "source": [
    "The performance can be evaluated by some common metricx, many are implemented in the MulticlassClassificationEvaluator previously used for the cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scorer = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol=\"indexedCategory\")\n",
    "f1_train = f1_score.evaluate(train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908e8d3",
   "metadata": {},
   "source": [
    "The code snippet below is used to generate the confusion matrices for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1758ef8-ea62-4a47-b1be-f30d0f89b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_conf_matrix(result, labs, title):\n",
    "    preds_and_labels = result.select(['prediction','indexedCategory']).orderBy('prediction')\n",
    "    metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "    conf_matrix = metrics.confusionMatrix().toArray()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(30, 20))\n",
    "    im = ax.imshow(conf_matrix)\n",
    "    ax.set_xticks(np.arange(len(labs)), labels=labs)\n",
    "    ax.set_yticks(np.arange(len(labs)), labels=labs)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    for i in range(len(labs)):\n",
    "        for j in range(len(labs)):\n",
    "            text = ax.text(j, i, conf_matrix[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "    \n",
    "    ax.set_title(\"title\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

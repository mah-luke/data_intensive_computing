---
title: "Report for Assignment 2"
subtitle: "Data-Intensive Computing SS2024"
author:
- Lukas Mahler (11908553)
- Julian Fl√ºr (11807481)
toc: True
---

\newpage

# Introduction
In this report we present our solution of the second assignment for the course Data-Intensive Computing,
which contained 3 main parts: 1. Redo the first assignment using RDDs, 2. Create a vectorized TF-IDF pipeline
using Spark ML and 3. Implement a SVM-based classifier on the pipeline of task 2 which predicts the category type
of a review.

# Problem Overview

## Data set

We are again working on a data set of Amazon reviews.
While there are 10 attributes, such as **helpful** or a **reviewTime**, we are only interested in two of them.
Namely:

- **category**: the category that the product belongs to
- **reviewText**: the content of the review; this is the text to be processed

Each review is in exactly one category and requires no preprocessing.

The review text on the other hand has to be split into unigrams where each token is one word, those
 tokens are used to calculate the TF-IDF for each review.
We split on whitespaces as well as a list of special characters and digits.
Also all the text is cast to lower case and certain stopwords are ignored for the analysis.

## Chi-square value

The chi-square value is a metric, expressing the dependence between a token and a category.
Essentially the more often a term is used in a category and the less it is used in reviews from other categories the more important it is.

$$\chi^2_{\text{tc}}=\frac{N(AD-BC)^2}{(A+B)(A+C)(B+D)(C+D)}$$

# Methodology and Approach
Spark is used with the following configuration when running on the server:
```python
spark: SparkSession = SparkSession.builder \
    .appName("cluster") \
    .config("spark.executor.instances", 435) \
    .getOrCreate()
sc: SparkContext = spark.sparkContext
sc.addPyFile(str(BASE_PATH / "src" / "exercise2.zip"))
```
Using 435 executor instances maximizes the synchronizity for the full dataset, allowing to reduce
the runtime of the notebook considerably.
As the cluster does not have our package installed, we zip it and add it to Spark with the
addPyFile method.
Adding that zip allows us to import our package when the notebook is executed on the cluster.

Development on the Devset is done locally (spark runs on our machine), to reduce the load of the cluster. Developing locally also simplifies debugging, as the code is executed by a single machine instead of being distributed amongst multiple workers.
Following configuration is used for local development:
```python
spark: SparkSession = SparkSession.builder \
    .appName("local") \
    .config("spark.driver.host", "localhost") \
    .config("spark.driver.bindAddress", "localhost") \
    .getOrCreate()
sc: SparkContext = spark.sparkContext
```

## Task 1: RDDs
We repeat the tasks of the first assignment, this time utilizing Sparks' RDDs.

### Load Dataset
First, the dataset is loaded, using the previously created SparkContext, by calling the
textFile method. This method loads the the given file line by line, then the map method is used to convert each line into a dictionary by passing in json.loads, which extracts a dictionary from a string in json format.

Now that we the RDD of our dataset loaded, we can calculate the number of reviews in the
dataset by calling the count method on the RDD, which we will need later to calculate the
Chi-square values.

### Number of reviews per category
We also directly count the number of reviews per category, similar to the last assignment, where we created 2 jobs: one to calculate the number of reviews per category, and another to calculate the Chi-square values, which used the results of the first job.

Category counts are calculated in the same manner as for the first job: For each review we map a tuple of ```(<category_name>, 1)```, which is reduced by ```reduceByKey(operator.add)```, which calculates the sum for each category (as the category is the key).
A dictionary of the calculation is retrieved by calling ```collectAsMap()```.

The stopwords are loaded from the src directory into a set, as a set in Python uses a hashmap as data structure, allowing to check if a value is contained in the set in O(1).

In order to obtain the Chi-square values, category counts per token ```(<token>, {<category>: <number_of_reviews>, ...})``` are calculated.
The list of tokens for each review is generated by splitting the reviewText with the same
method as in the last exercise: We replace the special splitting characters (specified in the specification of Assignment 1) with spaces, and then replace all characters surrounded by spaces (single character tokens) also with a space. The resulting text is then splitted by the space character, yielding the unfiltered tokens for the review. The unfiltered tokens are deduplicated by using Pythons' set. We iterate over this set and return a list of tuples for each review: ```[(<token>, <category>), ...]```.

The token retrieval described above is done inside the flatMap method, which yields each token and category combination separately, allowing to use Sparks' filter method instead of iterating over the tokens directly in Python, which potentially results in a better performance
due to the optimization possibilities for Spark.

Each token is filtered against the stopwords set using the filter method for RDDs and then mapped using the mapValues method: ```{<category>: 1}```. Mapping to a dictionary simplifies
merging them in the following reduction step.
We apply the reduceByKey method to the dictionaries for each key, by using the merge_dicts method we created for the last assignment.
The result of the merge_dicts method is a dictionary with the number of reviews (value) for each category (key).

### Chi-square calculation and filtering for top 75 tokens per category
In the previous steps we collected all information required to calculate the Chi-square values.
FlatMap on the RDD containing the number of reviews dictionary for each token is applied, which allows to calculate the Chi-square values for each token while also allowing change the indexing from per token to per category.
This is achieved by the calculate_chi_square_per_token method, which takes the category counts (number of reviews per category) for the current token, the category counts for the full dataset and the total number of reviews for the full dataset.

The Chi-square calculation is done in the same way as last assignment, filling in the formula with the calculated values:
```python
def calculate_chi_square_per_token(
    cur_category_counts: tuple[str, dict[str, int]],
    category_counts,
    reviews_cnt):
doc_cnt_term = sum(cur_category_counts[1].values())
for category, doc_cnt_cur_term in cur_category_counts[1].items():
    a = doc_cnt_cur_term
    b = doc_cnt_term - a
    c = category_counts[category] - a
    d = reviews_cnt - a - b - c
    yield category, (cur_category_counts[0],
        calculate_chi_squares(a,b,c,d,reviews_cnt))
```
The flatMap returns ```(<category>, (<token>, <Chi-square>))```, to which we then apply a groupByKey, which collects all values for the same category (key), and then apply ```mapValues(list)```, which maps the grouped values to a list of tuples.
We call mapValues on the result, in which we sort each list and then only return the top 75 tokens. As a last step, we sort the category order by applying sortByKey.

The 2 calls to mapValues could have been done in one step as well, which could potentially result in a slightly better performance.

### Top joined tokens and output generation
Last step, we produced the RDD with the top 75 tokens per category. This RDD is converted into the first part of the output text file by converting each list of tokens into strings as specified for the assignment and then joining them together with \\n.

Calculation of the overall top tokens is done by applying flatMap, which yields only each token, the category is omitted. Those tokens are then deduplicated by calling the distinct method.
The result is then sorted and collected to a list, to then be converted to a string and appended to the other result, which results in the final result, which is written to "output_rdd.txt".


## Task 2: Spark ML TF-IDF pipeline



## Task 3: SVM Classifier


# Conclusions


## Result



## Runtime

In the table below the runtime is given.

| task                           | runtime           |
|--------------------------------|------------------:|
| Task 1: Assignment 1 on RDDS   | 4 min 50 sec      |
| Task 2: TF-IDF pipeline        |                   |
| Task 3: SVM model              |                   |
